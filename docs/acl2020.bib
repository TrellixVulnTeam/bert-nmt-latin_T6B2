@article{tensor2tensor,
  author    = {Ashish Vaswani and Samy Bengio and Eugene Brevdo and
    Francois Chollet and Aidan N. Gomez and Stephan Gouws and Llion Jones and
    \L{}ukasz Kaiser and Nal Kalchbrenner and Niki Parmar and Ryan Sepassi and
    Noam Shazeer and Jakob Uszkoreit},
  title     = {Tensor2Tensor for Neural Machine Translation},
  journal   = {CoRR},
  volume    = {abs/1803.07416},
  year      = {2018},
  url       = {http://arxiv.org/abs/1803.07416},
}

@inproceedings{ott2019fairseq,
  title = {fairseq: A Fast, Extensible Toolkit for Sequence Modeling},
  author = {Myle Ott and Sergey Edunov and Alexei Baevski and Angela Fan and Sam Gross and Nathan Ng and David Grangier and Michael Auli},
  booktitle = {Proceedings of NAACL-HLT 2019: Demonstrations},
  year = {2019},
}

@inproceedings{dellorletta2014linguistic,
author = {Dell'Orletta, Felice and Wieling, Martijn and Cimino, Andrea and Venturi, Giulia and Montemagni, Simonetta},
year = {2014},
month = {06},
pages = {},
title = {Assessing the Readability of Sentences: Which Corpora and Features?},
doi = {10.3115/v1/W14-1820}
}

@article{DBLP:journals/corr/SutskeverVL14,
  author    = {Ilya Sutskever and
               Oriol Vinyals and
               Quoc V. Le},
  title     = {Sequence to Sequence Learning with Neural Networks},
  journal   = {CoRR},
  volume    = {abs/1409.3215},
  year      = {2014},
  url       = {http://arxiv.org/abs/1409.3215},
  archivePrefix = {arXiv},
  eprint    = {1409.3215},
  timestamp = {Mon, 13 Aug 2018 16:48:06 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/SutskeverVL14},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{vaswani2017attention,
    title={Attention Is All You Need},
    author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
    year={2017},
    eprint={1706.03762},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@misc{devlin2018bert,
    title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
    author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
    year={2018},
    eprint={1810.04805},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@misc{bahdanau2014neural,
    title={Neural Machine Translation by Jointly Learning to Align and Translate},
    author={Dzmitry Bahdanau and Kyunghyun Cho and Yoshua Bengio},
    year={2014},
    eprint={1409.0473},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@misc{cho2014learning,
    title={Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation},
    author={Kyunghyun Cho and Bart van Merrienboer and Caglar Gulcehre and Dzmitry Bahdanau and Fethi Bougares and Holger Schwenk and Yoshua Bengio},
    year={2014},
    eprint={1406.1078},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@misc{jozefowicz2016exploring,
    title={Exploring the Limits of Language Modeling},
    author={Rafal Jozefowicz and Oriol Vinyals and Mike Schuster and Noam Shazeer and Yonghui Wu},
    year={2016},
    eprint={1602.02410},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@misc{kuchaiev2017factorization,
    title={Factorization tricks for LSTM networks},
    author={Oleksii Kuchaiev and Boris Ginsburg},
    year={2017},
    eprint={1703.10722},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@Misc{johnson2014,
author = {Kyle P. Johnson et al.},
title = {CLTK: The Classical Language Toolkit},
howpublished = {\url{https://github.com/cltk/cltk}},
year = {2014--2019},
}

@misc{liu2019text,
    title={Text Summarization with Pretrained Encoders},
    author={Yang Liu and Mirella Lapata},
    year={2019},
    eprint={1908.08345},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@inproceedings{imamura-sumita-2019-recycling,
    title = "Recycling a Pre-trained {BERT} Encoder for Neural Machine Translation",
    author = "Imamura, Kenji  and
      Sumita, Eiichiro",
    booktitle = "Proceedings of the 3rd Workshop on Neural Generation and Translation",
    month = nov,
    year = "2019",
    address = "Hong Kong",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D19-5603",
    doi = "10.18653/v1/D19-5603",
    pages = "23--31",
    abstract = "In this paper, a pre-trained Bidirectional Encoder Representations from Transformers (BERT) model is applied to Transformer-based neural machine translation (NMT). In contrast to monolingual tasks, the number of unlearned model parameters in an NMT decoder is as huge as the number of learned parameters in the BERT model. To train all the models appropriately, we employ two-stage optimization, which first trains only the unlearned parameters by freezing the BERT model, and then fine-tunes all the sub-models. In our experiments, stable two-stage optimization was achieved, in contrast the BLEU scores of direct fine-tuning were extremely low. Consequently, the BLEU scores of the proposed method were better than those of the Transformer base model and the same model without pre-training. Additionally, we confirmed that NMT with the BERT encoder is more effective in low-resource settings.",
}

@misc{clinchant2019use,
    title={On the use of BERT for Neural Machine Translation},
    author={St√©phane Clinchant and Kweon Woo Jung and Vassilina Nikoulina},
    year={2019},
    eprint={1909.12744},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@misc{yang2019making,
    title={Towards Making the Most of BERT in Neural Machine Translation},
    author={Jiacheng Yang and Mingxuan Wang and Hao Zhou and Chengqi Zhao and Yong Yu and Weinan Zhang and Lei Li},
    year={2019},
    eprint={1908.05672},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@misc{lample2019crosslingual,
    title={Cross-lingual Language Model Pretraining},
    author={Guillaume Lample and Alexis Conneau},
    year={2019},
    eprint={1901.07291},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@InProceedings{song2019mass,
  title = 	 {{MASS}: Masked Sequence to Sequence Pre-training for Language Generation},
  author = 	 {Song, Kaitao and Tan, Xu and Qin, Tao and Lu, Jianfeng and Liu, Tie-Yan},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {5926--5936},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Long Beach, California, USA},
  month = 	 {09--15 Jun},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/song19d/song19d.pdf},
  url = 	 {http://proceedings.mlr.press/v97/song19d.html},
}

@misc{liu2019roberta,
    title={RoBERTa: A Robustly Optimized BERT Pretraining Approach},
    author={Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
    year={2019},
    eprint={1907.11692},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@misc{dai2019transformerxl,
    title={Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context},
    author={Zihang Dai and Zhilin Yang and Yiming Yang and Jaime Carbonell and Quoc V. Le and Ruslan Salakhutdinov},
    year={2019},
    eprint={1901.02860},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@article{vig2019transformervis,
  author    = {Jesse Vig},
  title     = {A Multiscale Visualization of Attention in the Transformer Model},
  journal   = {arXiv preprint arXiv:1906.05714},
  year      = {2019},
  url       = {https://arxiv.org/abs/1906.05714}
}

@inproceedings{papineni-etal-2002-bleu,
    title = "{B}leu: a Method for Automatic Evaluation of Machine Translation",
    author = "Papineni, Kishore  and
      Roukos, Salim  and
      Ward, Todd  and
      Zhu, Wei-Jing",
    booktitle = "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2002",
    address = "Philadelphia, Pennsylvania, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P02-1040",
    doi = "10.3115/1073083.1073135",
    pages = "311--318",
}



@inproceedings{snoek2015scalable,
  title={Scalable bayesian optimization using deep neural networks},
  author={Snoek, Jasper and Rippel, Oren and Swersky, Kevin and Kiros, Ryan and Satish, Nadathur and Sundaram, Narayanan and Patwary, Mostofa and Prabhat, Mr and Adams, Ryan},
  booktitle={International conference on machine learning},
  pages={2171--2180},
  year={2015}
}


@article{bergstra2015hyperopt,
  title={Hyperopt: a python library for model selection and hyperparameter optimization},
  author={Bergstra, James and Komer, Brent and Eliasmith, Chris and Yamins, Dan and Cox, David D},
  journal={Computational Science \& Discovery},
  volume={8},
  number={1},
  pages={014008},
  year={2015},
  publisher={IOP Publishing}
}

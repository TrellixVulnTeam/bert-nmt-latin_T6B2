{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT-latin-v2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1RT3Jc-l7FU",
        "colab_type": "text"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8cT-DyMl3lp",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RpcQ1frF_6p",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KbSa6KLUeW8",
        "colab_type": "code",
        "outputId": "d1d32113-e84d-4bd6-d6e3-782096914e8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!pwd"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esK8-9U-21uZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjdvfWVE232r",
        "colab_type": "code",
        "outputId": "5ace0700-0a64-4bbc-bc1a-d081881417ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLZo9qd03ApJ",
        "colab_type": "code",
        "outputId": "4d9f7ccf-be0c-46bc-bdcb-c906837ebe8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "!ls /content/drive/My\\ Drive/shreya_bert_on_collab/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "checkpoints_bert_de_base\t   checkpoints_bert_lat_multilingual_2\n",
            "checkpoints_bert_lat_multilingual  checkpoints_bert_lat_multilingual_3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3rb-iHoUjJr",
        "colab_type": "code",
        "outputId": "a477083c-1ec8-470c-ecf4-f3e524d0ac28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon Dec  9 22:58:05 2019       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.36       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   44C    P8     7W /  75W |      0MiB /  7611MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BsXs8HIhl9P4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!rm -rf bert-nmt-latin"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpM_SWPOIXpl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/shreyapandit/bert-nmt-latin.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DA-wsBuJH3y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mv  bert-nmt-latin/* ."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfzJSEt7HahT",
        "colab_type": "text"
      },
      "source": [
        "#TODO: Upload latin files to translation folder (folder name: /content/examples/translation/lat-en_data)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCOmP9TZEXS1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir /content/examples/translation/lat-en_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgbpRxEQEXXv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##TODO: Make sure you upload the files now before proceeding next"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33W76uuK__wj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbbKiccAAAGX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLbRXLpUVRYX",
        "colab_type": "code",
        "outputId": "a227f34f-900f-4ddb-c89a-02393ae1df13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "!ls /content/examples/translation/lat-en_data"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test.tags.lat-en.en   train.tags.lat-en.en\n",
            "test.tags.lat-en.lat  train.tags.lat-en.lat\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6in_wdNiz8C",
        "colab_type": "code",
        "outputId": "c7d6a05d-b3c3-41d9-9ca5-b963fc4d7821",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "!ls /content/examples/translation/lat-en_data"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test.tags.lat-en.en   train.tags.lat-en.en\n",
            "test.tags.lat-en.lat  train.tags.lat-en.lat\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-AZZjs1mVwkg",
        "colab_type": "code",
        "outputId": "0ee8e1e7-6691-4405-c296-da66a805a16c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "!ls /content/examples/translation/iwslt14.tokenized.de-en/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "code\t\t\ttest.en-de.de.idx\t train.en-de.en.bin\n",
            "dict.de.txt\t\ttest.en-de.en.bin\t train.en-de.en.idx\n",
            "dict.en.txt\t\ttest.en-de.en.idx\t valid.bert.de-en\n",
            "makedataforbert.sh\ttmp\t\t\t valid.bert.en\n",
            "preprocess.py\t\ttrain.bert.de-en\t valid.bert.en-de.en.bin\n",
            "test.bert.de-en\t\ttrain.bert.en\t\t valid.bert.en-de.en.idx\n",
            "test.bert.en\t\ttrain.bert.en-de.en.bin  valid.de\n",
            "test.bert.en-de.en.bin\ttrain.bert.en-de.en.idx  valid.en\n",
            "test.bert.en-de.en.idx\ttrain.de\t\t valid.en-de.de.bin\n",
            "test.de\t\t\ttrain.en\t\t valid.en-de.de.idx\n",
            "test.en\t\t\ttrain.en-de.de.bin\t valid.en-de.en.bin\n",
            "test.en-de.de.bin\ttrain.en-de.de.idx\t valid.en-de.en.idx\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmIT1UtvNbqu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp  examples/translation/iwslt14.tokenized.de-en/makedataforbert.sh examples/translation/makedataforbert.sh "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvdyXflxVYde",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mv examples/translation/iwslt14.tokenized.de-en examples/translation/iwslt14.tokenized.de-en_backup"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BedEcntAIgTb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !rm /content/examples/translation/iwslt14.tokenized.de-en/tmp/*\n",
        "# !rm /content/examples/translation/iwslt14.tokenized.de-en/*"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQ3V_RR_JAOW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!ls examples/translation/iwslt14.tokenized.de-en/tmp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6Vb1M6m5C8P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!mv examples/translation/iwslt14.tokenized.de-en examples/translation/iwslt14.tokenized.de-en_backup"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80UpG9fCN5o6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!cat examples/translation/iwslt14.tokenized.de-en/tmp/train.tags.de-en.clean.en"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFp4m1g4N5yW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!cat examples/translation/iwslt14.tokenized.de-en/tmp/train.tags.de-en.clean.de"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmaTKEh2OI4O",
        "colab_type": "code",
        "outputId": "521bc865-ca5f-44e5-a625-cf8301bb2f01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%writefile examples/translation/prepare-wmt14_shreya.sh\n",
        "\n",
        "#!/usr/bin/env bash\n",
        "#\n",
        "# Adapted from https://github.com/facebookresearch/MIXER/blob/master/prepareData.sh\n",
        "\n",
        "echo 'Cloning Moses github repository (for tokenization scripts)...'\n",
        "git clone https://github.com/moses-smt/mosesdecoder.git\n",
        "\n",
        "echo 'Cloning Subword NMT repository (for BPE pre-processing)...'\n",
        "git clone https://github.com/rsennrich/subword-nmt.git\n",
        "\n",
        "SCRIPTS=mosesdecoder/scripts\n",
        "TOKENIZER=$SCRIPTS/tokenizer/tokenizer.perl\n",
        "LC=$SCRIPTS/tokenizer/lowercase.perl\n",
        "CLEAN=$SCRIPTS/training/clean-corpus-n.perl\n",
        "BPEROOT=subword-nmt\n",
        "BPE_TOKENS=10000\n",
        "\n",
        "if [ ! -d \"$SCRIPTS\" ]; then\n",
        "    echo \"Please set SCRIPTS variable correctly to point to Moses scripts.\"\n",
        "    exit\n",
        "fi\n",
        "\n",
        "src=lat\n",
        "tgt=en\n",
        "lang=lat-en\n",
        "prep=iwslt14.tokenized.lat-en\n",
        "tmp=$prep/tmp\n",
        "orig=lat-en_data\n",
        "\n",
        "#mkdir -p $orig $tmp $prep\n",
        "mkdir -p $tmp $prep\n",
        "echo $orig $tmp $prep\n",
        "\n",
        "\n",
        "#cd ..\n",
        "\n",
        "echo \"pre-processing train data...\"\n",
        "for l in $src $tgt; do\n",
        "    f=train.tags.$lang.$l\n",
        "    tok=train.tags.$lang.tok.$l\n",
        "\n",
        "    cat $orig/$f | \\\n",
        "    perl $TOKENIZER -threads 8 -l $l > $tmp/$tok\n",
        "    echo \"\"\n",
        "\n",
        "\n",
        "done\n",
        "\n",
        "perl $CLEAN -ratio 1.5 $tmp/train.tags.$lang.tok $src $tgt $tmp/train.tags.$lang.clean 1 175\n",
        "for l in $src $tgt; do\n",
        "    perl $LC < $tmp/train.tags.$lang.clean.$l > $tmp/train.tags.$lang.$l\n",
        "done\n",
        "\n",
        "\n",
        "# the code above does good job with creating tokens and clean token , 6 files are created..\n",
        "# following code does something with test and validation set..\n",
        "\n",
        "\n",
        "echo \"pre-processing valid/test data...\"\n",
        "# for l in $src $tgt; do\n",
        "#     for o in `ls $orig/$lang/IWSLT14.TED*.$l.xml`; do\n",
        "#     fname=${o##*/}\n",
        "#     f=$tmp/${fname%.*}\n",
        "#     echo $o $f\n",
        "#     grep '<seg id' $o | \\\n",
        "#         sed -e 's/<seg id=\"[0-9]*\">\\s*//g' | \\\n",
        "#         sed -e 's/\\s*<\\/seg>\\s*//g' | \\\n",
        "#         sed -e \"s/\\’/\\'/g\" | \\\n",
        "#     perl $TOKENIZER -threads 8 -l $l | \\\n",
        "#     perl $LC > $f\n",
        "#     echo \"\"\n",
        "#     done\n",
        "# done\n",
        "\n",
        "for l in $src $tgt; do\n",
        "    f=test.tags.$lang.$l\n",
        "    tok=test.tags.$lang.tok.$l\n",
        "\n",
        "    cat $orig/$f | \\\n",
        "    perl $TOKENIZER -threads 8 -l $l > $tmp/$tok\n",
        "    echo \"\"\n",
        "\n",
        "done\n",
        "\n",
        "echo \"creating train, valid, test...\"\n",
        "for l in $src $tgt; do\n",
        "    awk '{if (NR%23 == 0)  print $0; }' $tmp/train.tags.$lang.$l > $tmp/valid.$l\n",
        "    awk '{if (NR%23 != 0)  print $0; }' $tmp/train.tags.$lang.$l > $tmp/train.$l\n",
        "\n",
        "    # cat $tmp/IWSLT14.TED.dev2010.de-en.$l \\\n",
        "    #     $tmp/IWSLT14.TEDX.dev2012.de-en.$l \\\n",
        "    #     $tmp/IWSLT14.TED.tst2010.de-en.$l \\\n",
        "    #     $tmp/IWSLT14.TED.tst2011.de-en.$l \\\n",
        "    #     $tmp/IWSLT14.TED.tst2012.de-en.$l \\\n",
        "    #     > $tmp/test.$l\n",
        "    cat $orig/test.tags.$lang.$l  > $tmp/test.$l\n",
        "\n",
        "done\n",
        "\n",
        "TRAIN=$tmp/train.en-de\n",
        "BPE_CODE=$prep/code\n",
        "rm -f $TRAIN\n",
        "for l in $src $tgt; do\n",
        "    cat $tmp/train.$l >> $TRAIN\n",
        "done\n",
        "\n",
        "echo \"learn_bpe.py on ${TRAIN}...\"\n",
        "python $BPEROOT/learn_bpe.py -s $BPE_TOKENS < $TRAIN > $BPE_CODE\n",
        "\n",
        "for L in $src $tgt; do\n",
        "    for f in train.$L valid.$L test.$L; do\n",
        "        echo \"apply_bpe.py to ${f}...\"\n",
        "        python $BPEROOT/apply_bpe.py -c $BPE_CODE < $tmp/$f > $prep/$f\n",
        "    done\n",
        "done"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing examples/translation/prepare-wmt14_shreya.sh\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HC_expOQOI8t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EV-6gslwN5wc",
        "colab_type": "code",
        "outputId": "908700f3-431e-4b31-ae2d-bad8bde3c2ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!cd examples/translation/ && sh ./prepare-wmt14_shreya.sh"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning Moses github repository (for tokenization scripts)...\n",
            "fatal: destination path 'mosesdecoder' already exists and is not an empty directory.\n",
            "Cloning Subword NMT repository (for BPE pre-processing)...\n",
            "Cloning into 'subword-nmt'...\n",
            "remote: Enumerating objects: 29, done.\u001b[K\n",
            "remote: Counting objects: 100% (29/29), done.\u001b[K\n",
            "remote: Compressing objects: 100% (22/22), done.\u001b[K\n",
            "remote: Total 538 (delta 10), reused 21 (delta 7), pack-reused 509\u001b[K\n",
            "Receiving objects: 100% (538/538), 226.98 KiB | 605.00 KiB/s, done.\n",
            "Resolving deltas: 100% (316/316), done.\n",
            "lat-en_data iwslt14.tokenized.lat-en/tmp iwslt14.tokenized.lat-en\n",
            "pre-processing train data...\n",
            "Tokenizer Version 1.1\n",
            "Language: lat\n",
            "Number of threads: 8\n",
            "WARNING: No known abbreviations for language 'lat', attempting fall-back to English version...\n",
            "\n",
            "Tokenizer Version 1.1\n",
            "Language: en\n",
            "Number of threads: 8\n",
            "\n",
            "clean-corpus.perl: processing iwslt14.tokenized.lat-en/tmp/train.tags.lat-en.tok.lat & .en to iwslt14.tokenized.lat-en/tmp/train.tags.lat-en.clean, cutoff 1-175, ratio 1.5\n",
            "......\n",
            "Input sentences: 60134  Output sentences:  21581\n",
            "pre-processing valid/test data...\n",
            "Tokenizer Version 1.1\n",
            "Language: lat\n",
            "Number of threads: 8\n",
            "WARNING: No known abbreviations for language 'lat', attempting fall-back to English version...\n",
            "\n",
            "Tokenizer Version 1.1\n",
            "Language: en\n",
            "Number of threads: 8\n",
            "\n",
            "creating train, valid, test...\n",
            "learn_bpe.py on iwslt14.tokenized.lat-en/tmp/train.en-de...\n",
            "subword-nmt/learn_bpe.py:267: DeprecationWarning: this script's location has moved to /content/examples/translation/subword-nmt/subword_nmt. This symbolic link will be removed in a future version. Please point to the new location, or install the package and use the command 'subword-nmt'\n",
            "  DeprecationWarning\n",
            "apply_bpe.py to train.lat...\n",
            "subword-nmt/apply_bpe.py:328: DeprecationWarning: this script's location has moved to /content/examples/translation/subword-nmt/subword_nmt. This symbolic link will be removed in a future version. Please point to the new location, or install the package and use the command 'subword-nmt'\n",
            "  DeprecationWarning\n",
            "subword-nmt/apply_bpe.py:345: ResourceWarning: unclosed file <_io.TextIOWrapper name='iwslt14.tokenized.lat-en/code' mode='r' encoding='UTF-8'>\n",
            "  args.codes = codecs.open(args.codes.name, encoding='utf-8')\n",
            "apply_bpe.py to valid.lat...\n",
            "subword-nmt/apply_bpe.py:328: DeprecationWarning: this script's location has moved to /content/examples/translation/subword-nmt/subword_nmt. This symbolic link will be removed in a future version. Please point to the new location, or install the package and use the command 'subword-nmt'\n",
            "  DeprecationWarning\n",
            "subword-nmt/apply_bpe.py:345: ResourceWarning: unclosed file <_io.TextIOWrapper name='iwslt14.tokenized.lat-en/code' mode='r' encoding='UTF-8'>\n",
            "  args.codes = codecs.open(args.codes.name, encoding='utf-8')\n",
            "apply_bpe.py to test.lat...\n",
            "subword-nmt/apply_bpe.py:328: DeprecationWarning: this script's location has moved to /content/examples/translation/subword-nmt/subword_nmt. This symbolic link will be removed in a future version. Please point to the new location, or install the package and use the command 'subword-nmt'\n",
            "  DeprecationWarning\n",
            "subword-nmt/apply_bpe.py:345: ResourceWarning: unclosed file <_io.TextIOWrapper name='iwslt14.tokenized.lat-en/code' mode='r' encoding='UTF-8'>\n",
            "  args.codes = codecs.open(args.codes.name, encoding='utf-8')\n",
            "apply_bpe.py to train.en...\n",
            "subword-nmt/apply_bpe.py:328: DeprecationWarning: this script's location has moved to /content/examples/translation/subword-nmt/subword_nmt. This symbolic link will be removed in a future version. Please point to the new location, or install the package and use the command 'subword-nmt'\n",
            "  DeprecationWarning\n",
            "subword-nmt/apply_bpe.py:345: ResourceWarning: unclosed file <_io.TextIOWrapper name='iwslt14.tokenized.lat-en/code' mode='r' encoding='UTF-8'>\n",
            "  args.codes = codecs.open(args.codes.name, encoding='utf-8')\n",
            "apply_bpe.py to valid.en...\n",
            "subword-nmt/apply_bpe.py:328: DeprecationWarning: this script's location has moved to /content/examples/translation/subword-nmt/subword_nmt. This symbolic link will be removed in a future version. Please point to the new location, or install the package and use the command 'subword-nmt'\n",
            "  DeprecationWarning\n",
            "subword-nmt/apply_bpe.py:345: ResourceWarning: unclosed file <_io.TextIOWrapper name='iwslt14.tokenized.lat-en/code' mode='r' encoding='UTF-8'>\n",
            "  args.codes = codecs.open(args.codes.name, encoding='utf-8')\n",
            "apply_bpe.py to test.en...\n",
            "subword-nmt/apply_bpe.py:328: DeprecationWarning: this script's location has moved to /content/examples/translation/subword-nmt/subword_nmt. This symbolic link will be removed in a future version. Please point to the new location, or install the package and use the command 'subword-nmt'\n",
            "  DeprecationWarning\n",
            "subword-nmt/apply_bpe.py:345: ResourceWarning: unclosed file <_io.TextIOWrapper name='iwslt14.tokenized.lat-en/code' mode='r' encoding='UTF-8'>\n",
            "  args.codes = codecs.open(args.codes.name, encoding='utf-8')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XJzO0cGN5tK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!ls -lah  --time-style=full-iso examples/translation/iwslt14.tokenized.lat-en/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TJQRVuD_9cL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!ls -lah  --time-style=full-iso examples/translation/iwslt14.tokenized.lat-en/tmp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OiUdkI6mBed-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!ls -lah  --time-style=full-iso examples/translation/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8m3BB9h2BeW_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp examples/translation/iwslt14.tokenized.de-en_backup/makedataforbert.sh \\\n",
        "      examples/translation/iwslt14.tokenized.lat-en/makedataforbert.sh"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nrzUc9osYmsm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7ldAiYOYm2T",
        "colab_type": "code",
        "outputId": "39463c2c-83c0-493d-fde4-ef64c7fc3a5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "!cd examples/translation/iwslt14.tokenized.lat-en && sh ./makedataforbert.sh lat"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "src lng lat\n",
            "Warning: No built-in rules for language lat.\n",
            "Detokenizer Version $Revision: 4134 $\n",
            "Language: lat\n",
            "Warning: No built-in rules for language lat.\n",
            "Detokenizer Version $Revision: 4134 $\n",
            "Language: lat\n",
            "Warning: No built-in rules for language lat.\n",
            "Detokenizer Version $Revision: 4134 $\n",
            "Language: lat\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9cW4Yoe67pg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjCdcAgWZh8c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!ls -lah  --time-style=full-iso examples/translation/iwslt14.tokenized.lat-en"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofcfeEqCZiDO",
        "colab_type": "code",
        "outputId": "34e985f7-9a8a-4512-8f9b-9116e243f2f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "!cd examples/translation/iwslt14.tokenized.lat-en && sh ./makedataforbert.sh en"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "src lng en\n",
            "Detokenizer Version $Revision: 4134 $\n",
            "Language: en\n",
            "Detokenizer Version $Revision: 4134 $\n",
            "Language: en\n",
            "Detokenizer Version $Revision: 4134 $\n",
            "Language: en\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMAnuloHZ2iP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!ls -lah  --time-style=full-iso examples/translation/iwslt14.tokenized.lat-en"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSzrxGCbZ2oU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JD9rx--DfQ2J",
        "colab_type": "code",
        "outputId": "c474f433-90bd-4f29-ce00-ae091aedc83c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        }
      },
      "source": [
        "!pip install --editable ."
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Obtaining file:///content\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.6/dist-packages (from fairseq==0.6.2) (1.13.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fairseq==0.6.2) (1.17.4)\n",
            "Collecting sacrebleu\n",
            "  Downloading https://files.pythonhosted.org/packages/45/31/1a135b964c169984b27fb2f7a50280fa7f8e6d9d404d8a9e596180487fd1/sacrebleu-1.4.3-py3-none-any.whl\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from fairseq==0.6.2) (1.3.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from fairseq==0.6.2) (4.28.1)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from fairseq==0.6.2) (1.10.32)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from fairseq==0.6.2) (2.21.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi->fairseq==0.6.2) (2.19)\n",
            "Requirement already satisfied: typing in /usr/local/lib/python3.6/dist-packages (from sacrebleu->fairseq==0.6.2) (3.6.6)\n",
            "Collecting portalocker\n",
            "  Downloading https://files.pythonhosted.org/packages/91/db/7bc703c0760df726839e0699b7f78a4d8217fdc9c7fcb1b51b39c5a22a4e/portalocker-1.5.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->fairseq==0.6.2) (0.2.1)\n",
            "Requirement already satisfied: botocore<1.14.0,>=1.13.32 in /usr/local/lib/python3.6/dist-packages (from boto3->fairseq==0.6.2) (1.13.32)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->fairseq==0.6.2) (0.9.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->fairseq==0.6.2) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->fairseq==0.6.2) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->fairseq==0.6.2) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->fairseq==0.6.2) (2019.11.28)\n",
            "Requirement already satisfied: python-dateutil<2.8.1,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.32->boto3->fairseq==0.6.2) (2.6.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.32->boto3->fairseq==0.6.2) (0.15.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<2.8.1,>=2.1; python_version >= \"2.7\"->botocore<1.14.0,>=1.13.32->boto3->fairseq==0.6.2) (1.12.0)\n",
            "Installing collected packages: portalocker, sacrebleu, fairseq\n",
            "  Running setup.py develop for fairseq\n",
            "Successfully installed fairseq portalocker-1.5.2 sacrebleu-1.4.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOu1I7uufQ6u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!rm  /content/examples/translation/iwslt14.tokenized.lat-en/*"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpTWbq4nfRB2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp examples/translation/iwslt14.tokenized.de-en_backup/preprocess.py \\\n",
        "      examples/translation/iwslt14.tokenized.lat-en/preprocess.py\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_jfDoKsHwbU",
        "colab_type": "code",
        "outputId": "6c5867bd-a41c-4af2-a050-64797f82c171",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Remove if running again..\n",
        "!rm /content/examples/translation/iwslt14.tokenized.lat-en/dict.en.txt\n",
        "!rm /content/examples/translation/iwslt14.tokenized.lat-en/dict.lat.txt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove '/content/examples/translation/iwslt14.tokenized.lat-en/dict.en.txt': No such file or directory\n",
            "rm: cannot remove '/content/examples/translation/iwslt14.tokenized.lat-en/dict.lat.txt': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BqPbS434fQ-5",
        "colab_type": "code",
        "outputId": "5646463e-f521-49cc-d678-ff55ada00a2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        }
      },
      "source": [
        "!cd examples/translation/iwslt14.tokenized.lat-en/ &&   python preprocess.py --source-lang en --target-lang lat \\\n",
        "  --trainpref train --validpref valid --testpref test \\\n",
        "  --destdir `pwd`  --joined-dictionary --bert-model-name bert-base-multilingual-uncased"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Namespace(alignfile=None, bert_model_name='bert-base-multilingual-uncased', cpu=False, criterion='cross_entropy', dataset_impl='cached', destdir='/content/examples/translation/iwslt14.tokenized.lat-en', fp16=False, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=True, log_format=None, log_interval=1000, lr_scheduler='fixed', memory_efficient_fp16=False, min_loss_scale=0.0001, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, only_source=False, optimizer='nag', padding_factor=8, seed=1, source_lang='en', srcdict=None, target_lang='lat', task='translation', tbmf_wrapper=False, tensorboard_logdir='', testpref='test', tgtdict=None, threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, trainpref='train', user_dir=None, validpref='valid', workers=1)\n",
            "{'train.en', 'train.lat'}\n",
            "| [en] Dictionary: 9727 types\n",
            "| [en] train.en: 20643 sents, 1342813 tokens, 0.0% replaced by <unk>\n",
            "| [en] Dictionary: 9727 types\n",
            "| [en] valid.en: 938 sents, 62438 tokens, 0.0% replaced by <unk>\n",
            "| [en] Dictionary: 9727 types\n",
            "| [en] test.en: 6869 sents, 788774 tokens, 4.73% replaced by <unk>\n",
            "| [lat] Dictionary: 9727 types\n",
            "| [lat] train.lat: 20643 sents, 1236832 tokens, 0.0% replaced by <unk>\n",
            "| [lat] Dictionary: 9727 types\n",
            "| [lat] valid.lat: 938 sents, 57800 tokens, 0.0% replaced by <unk>\n",
            "| [lat] Dictionary: 9727 types\n",
            "| [lat] test.lat: 6869 sents, 645220 tokens, 4.88% replaced by <unk>\n",
            "100% 871891/871891 [00:01<00:00, 789644.07B/s]\n",
            "| [en] Dictionary: 105878 types\n",
            "| [en] train.bert.en: 20643 sents, 1963461 tokens, 0.0% replaced by [UNK]\n",
            "| [en] Dictionary: 105878 types\n",
            "| [en] valid.bert.en: 938 sents, 91290 tokens, 0.0% replaced by [UNK]\n",
            "| [en] Dictionary: 105878 types\n",
            "| [en] test.bert.en: 6869 sents, 1446382 tokens, 0.0% replaced by [UNK]\n",
            "| Wrote preprocessed data to /content/examples/translation/iwslt14.tokenized.lat-en\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmXnYkXMaJXh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!ls /content/examples/translation/iwslt14.tokenized.lat-en/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MsoauhCThNAs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!ls -lah  --time-style=full-iso examples/translation/iwslt14.tokenized.lat-en/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1t0w74QMhNNP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!rm checkpoints"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hPaNVE0EhYsy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!ln -s  checkpoints/ /content/drive/My\\ Drive/shreya_bert_on_collab/checkpoints_bert_lat_multilingual"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTamZbsw6lRk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2cCXvUbJtxq",
        "colab_type": "code",
        "outputId": "cc8dd619-91de-457f-f53e-60743161fec2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%writefile training_script.sh\n",
        "#!/usr/bin/env bash\n",
        "nvidia-smi\n",
        "\n",
        "cd . #/yourpath/bertnmt\n",
        "python3 -c \"import torch; print(torch.__version__)\"\n",
        "\n",
        "src=en\n",
        "tgt=lat\n",
        "bedropout=0.5\n",
        "ARCH=transformer_s2_iwslt_de_en\n",
        "DATAPATH=examples/translation/iwslt14.tokenized.$tgt-$src\n",
        "#SAVEDIR=checkpoints/iwed_${src}_${tgt}_${bedropout}\n",
        "#SAVEDIR=/content/drive/My*/shreya_bert_on_collab/checkpoints_bert_lat_multilingual_2\n",
        "#mkdir -p $SAVEDIR\n",
        "mkdir /content/drive/My\\ Drive/shreya_bert_on_collab/checkpoints_bert_lat_multilingual_2 \n",
        "if [ ! -f $SAVEDIR/checkpoint_nmt.pt ]\n",
        "then\n",
        "    cp /your_pretrained_nmt_model $SAVEDIR/checkpoint_nmt.pt\n",
        "fi\n",
        "if [ ! -f \"$SAVEDIR/checkpoint_last.pt\" ]\n",
        "then\n",
        "## incorrectly coming to this\n",
        "echo \"commented code for warmup-from-nmt \"\n",
        "warmup=\"\"\n",
        "#warmup=\"--warmup-from-nmt --reset-lr-scheduler\"\n",
        "else\n",
        "warmup=\"\"\n",
        "fi\n",
        "\n",
        "\n",
        "python train.py $DATAPATH \\\n",
        "-a $ARCH --optimizer adam --lr 0.001 -s $src -t $tgt --label-smoothing 0.1 \\\n",
        "--dropout 0.3  --min-lr '1e-09' --lr-scheduler inverse_sqrt --weight-decay 0.0001 \\\n",
        "--criterion label_smoothed_cross_entropy --max-update 150000 --warmup-updates 4000 --warmup-init-lr '1e-07' \\\n",
        "--adam-betas '(0.9,0.98)' \\\n",
        "--save-dir /content/drive/My\\ Drive/shreya_bert_on_collab/checkpoints_bert_lat_multilingual_2 \\\n",
        "\\--share-all-embeddings $warmup \\\n",
        "--encoder-bert-dropout --encoder-bert-dropout-ratio $bedropout \\\n",
        "--save-interval 5 --max-epoch 20  --bert-model-name bert-base-multilingual-uncased --max-tokens 4000 \\\n",
        " | tee -a /content/drive/My\\ Drive/shreya_bert_on_collab/checkpoints_bert_lat_multilingual_2/training.log"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting training_script.sh\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4w32bYAb0H0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3O_xpamJ1iK",
        "colab_type": "code",
        "outputId": "9c0373f0-226e-4a6a-dd13-5e7eb8f2d1df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "!sh ./training_script.sh"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon Dec  9 15:30:41 2019       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.33.01    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   65C    P8    34W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "1.3.1\n",
            "mkdir: cannot create directory ‘/content/drive/My Drive/shreya_bert_on_collab/checkpoints_bert_lat_multilingual_2’: File exists\n",
            "cp: cannot stat '/your_pretrained_nmt_model': No such file or directory\n",
            "commented code for warmup-from-nmt \n",
            "Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, arch='transformer_s2_iwslt_de_en', attention_dropout=0.0, bert_first=True, bert_gates=[1, 1, 1, 1, 1, 1], bert_model_name='bert-base-multilingual-uncased', bert_output_layer=-1, bert_ratio=1.0, bucket_cap_mb=25, clip_norm=25, cpu=False, criterion='label_smoothed_cross_entropy', curriculum=0, data='examples/translation/iwslt14.tokenized.lat-en', dataset_impl='cached', ddp_backend='c10d', decoder_attention_heads=4, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=1024, decoder_input_dim=512, decoder_layers=6, decoder_learned_pos=False, decoder_no_bert=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, encoder_attention_heads=4, encoder_bert_dropout=True, encoder_bert_dropout_ratio=0.5, encoder_bert_mixup=False, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=1024, encoder_layers=6, encoder_learned_pos=False, encoder_normalize_before=False, encoder_ratio=1.0, find_unused_parameters=False, finetune_bert=False, fix_batches_to_gpus=False, fp16=False, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, lazy_load=False, left_pad_source='True', left_pad_target='False', log_format=None, log_interval=1000, lr=[0.001], lr_scheduler='inverse_sqrt', mask_cls_sep=False, max_epoch=20, max_sentences=None, max_sentences_valid=None, max_source_positions=1024, max_target_positions=1024, max_tokens=4000, max_update=150000, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=1e-09, no_epoch_checkpoints=False, no_progress_bar=False, no_save=False, no_token_positional_embeddings=False, num_workers=0, optimizer='adam', optimizer_overrides='{}', raw_text=False, required_batch_size_multiple=8, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='/content/drive/My Drive/shreya_bert_on_collab/checkpoints_bert_lat_multilingual_2', save_interval=5, save_interval_updates=0, seed=1, sentence_avg=False, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, source_lang='en', target_lang='lat', task='translation', tbmf_wrapper=False, tensorboard_logdir='', threshold_loss_scale=None, train_subset='train', update_freq=[1], upsample_primary=1, user_dir=None, valid_subset='valid', validate_interval=1, warmup_from_nmt=False, warmup_init_lr=1e-07, warmup_nmt_file='checkpoint_nmt.pt', warmup_updates=4000, weight_decay=0.0001)\n",
            "| [en] dictionary: 9728 types\n",
            "| [lat] dictionary: 9728 types\n",
            "| examples/translation/iwslt14.tokenized.lat-en valid en-lat 938 examples\n",
            "bert_gates [True, True, True, True, True, True]\n",
            "TransformerS2Model(\n",
            "  (encoder): TransformerS2Encoder(\n",
            "    (embed_tokens): Embedding(9728, 512, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0): TransformerS2EncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (bert_attn): MultiheadAttention(\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=1024, bias=True)\n",
            "        (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (1): TransformerS2EncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (bert_attn): MultiheadAttention(\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=1024, bias=True)\n",
            "        (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (2): TransformerS2EncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (bert_attn): MultiheadAttention(\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=1024, bias=True)\n",
            "        (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (3): TransformerS2EncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (bert_attn): MultiheadAttention(\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=1024, bias=True)\n",
            "        (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (4): TransformerS2EncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (bert_attn): MultiheadAttention(\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=1024, bias=True)\n",
            "        (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (5): TransformerS2EncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (bert_attn): MultiheadAttention(\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=1024, bias=True)\n",
            "        (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (decoder): TransformerDecoder(\n",
            "    (embed_tokens): Embedding(9728, 512, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0): TransformerDecoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (bert_attn): MultiheadAttention(\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=1024, bias=True)\n",
            "        (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (1): TransformerDecoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (bert_attn): MultiheadAttention(\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=1024, bias=True)\n",
            "        (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (2): TransformerDecoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (bert_attn): MultiheadAttention(\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=1024, bias=True)\n",
            "        (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (3): TransformerDecoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (bert_attn): MultiheadAttention(\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=1024, bias=True)\n",
            "        (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (4): TransformerDecoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (bert_attn): MultiheadAttention(\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=1024, bias=True)\n",
            "        (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (5): TransformerDecoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (bert_attn): MultiheadAttention(\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=1024, bias=True)\n",
            "        (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (bert_encoder): BertModel(\n",
            "    (embeddings): BertEmbeddings(\n",
            "      (word_embeddings): Embedding(105879, 768, padding_idx=0)\n",
            "      (position_embeddings): Embedding(512, 768)\n",
            "      (token_type_embeddings): Embedding(2, 768)\n",
            "      (LayerNorm): BertLayerNorm()\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (encoder): BertEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): BertLayerNorm()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): BertLayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (1): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): BertLayerNorm()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): BertLayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (2): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): BertLayerNorm()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): BertLayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (3): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): BertLayerNorm()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): BertLayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (4): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): BertLayerNorm()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): BertLayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (5): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): BertLayerNorm()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): BertLayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (6): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): BertLayerNorm()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): BertLayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (7): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): BertLayerNorm()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): BertLayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (8): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): BertLayerNorm()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): BertLayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (9): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): BertLayerNorm()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): BertLayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (10): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): BertLayerNorm()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): BertLayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (11): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): BertLayerNorm()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): BertLayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (pooler): BertPooler(\n",
            "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (activation): Tanh()\n",
            "    )\n",
            "  )\n",
            "\n",
            "| model transformer_s2_iwslt_de_en, criterion LabelSmoothedCrossEntropyCriterion\n",
            "| num. model params: 219633664 (num. trained: 52277248)\n",
            "| training on 1 GPUs\n",
            "| max tokens per GPU = 4000 and max sentences per GPU = None\n",
            "| loaded checkpoint /content/drive/My Drive/shreya_bert_on_collab/checkpoints_bert_lat_multilingual_2/checkpoint_last.pt (epoch 10 @ 6500 updates)\n",
            "| loading train data for epoch 10\n",
            "| examples/translation/iwslt14.tokenized.lat-en train en-lat 20643 examples\n",
            "| epoch 011 | loss 10.508 | nll_loss 10.001 | ppl 1024.53 | wps 1775 | ups 1 | wpb 1902.818 | bsz 31.758 | num_updates 7150 | lr 0.000747958 | gnorm 1.366 | clip 0.000 | oom 0.000 | wall 698 | train_wall 2775\n",
            "| epoch 011 | valid on 'valid' subset | loss 11.279 | nll_loss 10.831 | ppl 1821.63 | num_updates 7150 | best_loss 10.6567\n",
            "| epoch 012 | loss 10.429 | nll_loss 9.913 | ppl 963.90 | wps 1781 | ups 1 | wpb 1902.818 | bsz 31.758 | num_updates 7800 | lr 0.000716115 | gnorm 1.313 | clip 0.000 | oom 0.000 | wall 1415 | train_wall 3464\n",
            "| epoch 012 | valid on 'valid' subset | loss 11.036 | nll_loss 10.544 | ppl 1492.85 | num_updates 7800 | best_loss 10.6567\n",
            "| epoch 013 | loss 10.300 | nll_loss 9.765 | ppl 870.20 | wps 1773 | ups 1 | wpb 1902.818 | bsz 31.758 | num_updates 8450 | lr 0.000688021 | gnorm 1.348 | clip 0.000 | oom 0.000 | wall 2134 | train_wall 4156\n",
            "| epoch 013 | valid on 'valid' subset | loss 10.900 | nll_loss 10.389 | ppl 1340.76 | num_updates 8450 | best_loss 10.6567\n",
            "| epoch 014 | loss 10.230 | nll_loss 9.685 | ppl 823.17 | wps 1775 | ups 1 | wpb 1902.818 | bsz 31.758 | num_updates 9100 | lr 0.000662994 | gnorm 1.368 | clip 0.000 | oom 0.000 | wall 2853 | train_wall 4846\n",
            "| epoch 014 | valid on 'valid' subset | loss 11.496 | nll_loss 11.009 | ppl 2061.10 | num_updates 9100 | best_loss 10.6567\n",
            "| epoch 015 | loss 10.088 | nll_loss 9.524 | ppl 736.17 | wps 1777 | ups 1 | wpb 1902.818 | bsz 31.758 | num_updates 9750 | lr 0.000640513 | gnorm 1.482 | clip 0.000 | oom 0.000 | wall 3570 | train_wall 5536\n",
            "| epoch 015 | valid on 'valid' subset | loss 11.101 | nll_loss 10.588 | ppl 1538.86 | num_updates 9750 | best_loss 10.6567\n",
            "| saved checkpoint /content/drive/My Drive/shreya_bert_on_collab/checkpoints_bert_lat_multilingual_2/checkpoint15.pt (epoch 15 @ 9750 updates) (writing took 12.438968181610107 seconds)\n",
            "| epoch 016 | loss 10.035 | nll_loss 9.460 | ppl 704.21 | wps 1785 | ups 1 | wpb 1902.818 | bsz 31.758 | num_updates 10400 | lr 0.000620174 | gnorm 1.496 | clip 0.000 | oom 0.000 | wall 4297 | train_wall 6224\n",
            "| epoch 016 | valid on 'valid' subset | loss 10.954 | nll_loss 10.472 | ppl 1419.90 | num_updates 10400 | best_loss 10.6567\n",
            "| epoch 017 | loss 9.909 | nll_loss 9.316 | ppl 637.45 | wps 1769 | ups 1 | wpb 1902.818 | bsz 31.758 | num_updates 11050 | lr 0.000601657 | gnorm 1.563 | clip 0.000 | oom 0.000 | wall 5018 | train_wall 6917\n",
            "| epoch 017 | valid on 'valid' subset | loss 10.779 | nll_loss 10.274 | ppl 1238.53 | num_updates 11050 | best_loss 10.6567\n",
            "| epoch 018 | loss 9.915 | nll_loss 9.322 | ppl 640.24 | wps 1773 | ups 1 | wpb 1902.818 | bsz 31.758 | num_updates 11700 | lr 0.000584705 | gnorm 1.605 | clip 0.000 | oom 0.000 | wall 5738 | train_wall 7609\n",
            "| epoch 018 | valid on 'valid' subset | loss 10.890 | nll_loss 10.424 | ppl 1373.70 | num_updates 11700 | best_loss 10.6567\n",
            "| epoch 019 | loss 9.823 | nll_loss 9.216 | ppl 594.89 | wps 1769 | ups 1 | wpb 1902.818 | bsz 31.758 | num_updates 12350 | lr 0.00056911 | gnorm 1.663 | clip 0.000 | oom 0.000 | wall 6459 | train_wall 8302\n",
            "| epoch 019 | valid on 'valid' subset | loss 10.773 | nll_loss 10.268 | ppl 1233.28 | num_updates 12350 | best_loss 10.6567\n",
            "| epoch 020 | loss 9.768 | nll_loss 9.153 | ppl 569.39 | wps 1770 | ups 1 | wpb 1902.818 | bsz 31.758 | num_updates 13000 | lr 0.0005547 | gnorm 1.724 | clip 0.000 | oom 0.000 | wall 7179 | train_wall 8995\n",
            "| epoch 020 | valid on 'valid' subset | loss 10.808 | nll_loss 10.298 | ppl 1259.22 | num_updates 13000 | best_loss 10.6567\n",
            "| saved checkpoint /content/drive/My Drive/shreya_bert_on_collab/checkpoints_bert_lat_multilingual_2/checkpoint20.pt (epoch 20 @ 13000 updates) (writing took 12.026100397109985 seconds)\n",
            "| done training in 7211.6 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WX4oK9qCZC-I",
        "colab": {}
      },
      "source": [
        "#!ls -lah checkpoints/iwed_en_lat_0.5/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8eYWGZN8g9TR",
        "colab_type": "code",
        "outputId": "06d5e545-3e81-43a9-b271-1341d9ca3279",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "!python generate.py  --quiet --bert-model-name bert-base-multilingual-uncased \\\n",
        "                    --path /content/drive/My\\ Drive/shreya_bert_on_collab/checkpoints_bert_lat_multilingual_2/checkpoint_best.pt \\\n",
        "                    examples/translation/iwslt14.tokenized.lat-en \\\n",
        "                    | tee -a /content/drive/My\\ Drive/shreya_bert_on_collab/checkpoints_bert_lat_multilingual_2/generate.log"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Namespace(beam=5, bert_first=True, bert_gates=[1, 1, 1, 1, 1, 1], bert_model_name='bert-base-multilingual-uncased', bert_output_layer=-1, bert_ratio=1.0, change_ratio=False, cpu=False, criterion='cross_entropy', data='examples/translation/iwslt14.tokenized.lat-en', dataset_impl='cached', decoder_no_bert=False, diverse_beam_groups=-1, diverse_beam_strength=0.5, encoder_bert_dropout=False, encoder_bert_dropout_ratio=0.25, encoder_bert_mixup=False, encoder_ratio=1.0, finetune_bert=False, force_anneal=None, fp16=False, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', lazy_load=False, left_pad_source='True', left_pad_target='False', lenpen=1, log_format=None, log_interval=1000, lr_scheduler='fixed', lr_shrink=0.1, mask_cls_sep=False, match_source_len=False, max_len_a=0, max_len_b=200, max_sentences=None, max_source_positions=1024, max_target_positions=1024, max_tokens=12000, memory_efficient_fp16=False, min_len=1, min_loss_scale=0.0001, model_overrides='{}', momentum=0.99, nbest=1, no_beamable_mm=False, no_early_stop=False, no_progress_bar=False, no_repeat_ngram_size=0, num_shards=1, num_workers=0, optimizer='nag', path='/content/drive/My Drive/shreya_bert_on_collab/checkpoints_bert_lat_multilingual_2/checkpoint_best.pt', prefix_size=0, print_alignment=False, quiet=True, raw_text=False, remove_bpe=None, replace_unk=None, required_batch_size_multiple=8, results_path=None, sacrebleu=False, sampling=False, sampling_topk=-1, score_reference=False, seed=1, shard_id=0, skip_invalid_size_inputs_valid_test=False, source_lang=None, target_lang=None, task='translation', tbmf_wrapper=False, temperature=1.0, tensorboard_logdir='', threshold_loss_scale=None, unkpen=0, unnormalized=False, upsample_primary=1, user_dir=None, warmup_from_nmt=False, warmup_nmt_file='checkpoint_nmt.pt', warmup_updates=0, weight_decay=0.0)\n",
            "| [en] dictionary: 9728 types\n",
            "| [lat] dictionary: 9728 types\n",
            "| examples/translation/iwslt14.tokenized.lat-en test en-lat 6869 examples\n",
            "| loading model(s) from /content/drive/My Drive/shreya_bert_on_collab/checkpoints_bert_lat_multilingual_2/checkpoint_best.pt\n",
            "bert_gates [True, True, True, True, True, True]\n",
            "| Translated 6869 sentences (1357458 tokens) in 4436.2s (1.55 sentences/s, 305.99 tokens/s)\n",
            "| Generate test with beam=5: BLEU4 = 0.07, 3.2/0.0/0.0/0.0 (BP=1.000, ratio=2.116, syslen=1350589, reflen=638351)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O24CLXUAg9aF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZT5-tyx92dDp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLf-gV8m2dNy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ruM7ImPL2det",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%writefile training_script.sh\n",
        "#!/usr/bin/env bash\n",
        "nvidia-smi\n",
        "\n",
        "cd . #/yourpath/bertnmt\n",
        "python3 -c \"import torch; print(torch.__version__)\"\n",
        "\n",
        "src=en\n",
        "tgt=lat\n",
        "bedropout=0.5\n",
        "ARCH=transformer_s2_iwslt_de_en\n",
        "DATAPATH=examples/translation/iwslt14.tokenized.$tgt-$src\n",
        "#SAVEDIR=checkpoints/iwed_${src}_${tgt}_${bedropout}\n",
        "#SAVEDIR=/content/drive/My*/shreya_bert_on_collab/checkpoints_bert_lat_multilingual_2\n",
        "#mkdir -p $SAVEDIR\n",
        "mkdir /content/drive/My\\ Drive/shreya_bert_on_collab/checkpoints_bert_lat_multilingual_2 \n",
        "if [ ! -f $SAVEDIR/checkpoint_nmt.pt ]\n",
        "then\n",
        "    cp /your_pretrained_nmt_model $SAVEDIR/checkpoint_nmt.pt\n",
        "fi\n",
        "if [ ! -f \"$SAVEDIR/checkpoint_last.pt\" ]\n",
        "then\n",
        "## incorrectly coming to this\n",
        "echo \"commented code for warmup-from-nmt \"\n",
        "warmup=\"\"\n",
        "#warmup=\"--warmup-from-nmt --reset-lr-scheduler\"\n",
        "else\n",
        "warmup=\"\"\n",
        "fi\n",
        "\n",
        "python train.py $DATAPATH \\\n",
        "-a $ARCH --optimizer adam --lr 0.0005 -s $src -t $tgt --label-smoothing 0.1 \\\n",
        "--dropout 0.3  --min-lr '1e-09' --lr-scheduler inverse_sqrt --weight-decay 0.0001 \\\n",
        "--criterion label_smoothed_cross_entropy --max-update 150000 --warmup-updates 4000 --warmup-init-lr '1e-07' \\\n",
        "--adam-betas '(0.9,0.98)' \\\n",
        "--save-dir /content/drive/My\\ Drive/shreya_bert_on_collab/checkpoints_bert_lat_multilingual_2 \\\n",
        "\\--share-all-embeddings $warmup \\\n",
        "--encoder-bert-dropout --encoder-bert-dropout-ratio $bedropout \\\n",
        "--save-interval 5 --max-epoch 40  --bert-model-name bert-base-multilingual-uncased --max-tokens 4000 | tee -a $SAVEDIR/training.log"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5C1JdP0BIcsJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cq5p9obpIcoM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!sh ./training_script.sh"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wzyBZtfIcko",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python generate.py  --quiet --bert-model-name bert-base-multilingual-uncased \\\n",
        "                    --path /content/checkpoints/iwed_en_lat_0.5/checkpoint_best.pt \\\n",
        "                    examples/translation/iwslt14.tokenized.lat-en"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REJWJarOIchG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7xJa8nw2dbE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URFrnMrb2dK6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ix37alQOktaj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqK-rS15g9r5",
        "colab_type": "code",
        "outputId": "6cefa471-568a-472c-924f-c43ce7c235e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!wc -l examples/translation/iwslt14.tokenized.ita-en_backup/train.en"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "160239 examples/translation/iwslt14.tokenized.de-en_backup/train.en\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xt0oQmm-OdW8",
        "colab_type": "code",
        "outputId": "8ab36fad-3ca1-4fcf-f636-8112a874c37e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!wc -l examples/translation/iwslt14.tokenized.ita-en_backup/test.en"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6750 examples/translation/iwslt14.tokenized.de-en_backup/test.en\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLK1PAPARtcg",
        "colab_type": "code",
        "outputId": "98e5bcef-0aa6-49e6-e7a3-1cdcd6d0bcd9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 681
        }
      },
      "source": [
        "!while true; do date -uIs ; sleep 5m ; done"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-12-08T21:58:32+00:00\n",
            "2019-12-08T22:03:32+00:00\n",
            "2019-12-08T22:08:32+00:00\n",
            "2019-12-08T22:13:32+00:00\n",
            "2019-12-08T22:18:32+00:00\n",
            "2019-12-08T22:23:32+00:00\n",
            "2019-12-08T22:28:32+00:00\n",
            "2019-12-08T22:33:32+00:00\n",
            "2019-12-08T22:38:32+00:00\n",
            "2019-12-08T22:43:32+00:00\n",
            "2019-12-08T22:48:32+00:00\n",
            "2019-12-08T22:53:32+00:00\n",
            "2019-12-08T22:58:32+00:00\n",
            "2019-12-08T23:03:32+00:00\n",
            "2019-12-08T23:08:32+00:00\n",
            "2019-12-08T23:13:32+00:00\n",
            "2019-12-08T23:18:32+00:00\n",
            "2019-12-08T23:23:32+00:00\n",
            "2019-12-08T23:28:32+00:00\n",
            "2019-12-08T23:33:32+00:00\n",
            "2019-12-08T23:38:32+00:00\n",
            "2019-12-08T23:43:32+00:00\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-74-dbd4d95efa57>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'while true; do date -uIs ; sleep 5m ; done'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'also_return_output'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_system_commands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_system_compat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint:disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpip_warn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_system_compat\u001b[0;34m(shell, cmd, also_return_output)\u001b[0m\n\u001b[1;32m    436\u001b[0m   \u001b[0;31m# stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m   result = _run_command(\n\u001b[0;32m--> 438\u001b[0;31m       shell.var_expand(cmd, depth=2), clear_streamed_output=False)\n\u001b[0m\u001b[1;32m    439\u001b[0m   \u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_ns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_exit_code'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_INTERRUPTED_SIGNALS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_run_command\u001b[0;34m(cmd, clear_streamed_output)\u001b[0m\n\u001b[1;32m    193\u001b[0m       \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild_pty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0m_monitor_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_pty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_stdin_widget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0mepoll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_display_stdin_widget\u001b[0;34m(delay_millis)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m   \u001b[0mhide_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'cell_remove_stdin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m   \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocking_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mhide_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent_header\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    169\u001b[0m   \u001b[0;31m# unique.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m   \u001b[0mrequest_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msend_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     if (reply.get('type') == 'colab_reply' and\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a01GRMv6FFel",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}